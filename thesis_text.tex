%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                          
% Bakalářská práce                 
% Jiří Holuša                          
%                                          
% Jazyk: čeština
% Kódování: UTF-8
% Použitý styl: fithesis2
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------ Konfigurace -------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Load document class fithesis2
%% {10pt, 11pt, 12pt}
%% {draft, final}
%% {oneside, twoside}
%% {onecolumn, twocolumn}
\documentclass[11pt,oneside]{fithesis2}


%% Basic packages
\usepackage{lmodern}
\usepackage[czech]{babel}
\usepackage{cmap}
\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings} 


%% Additional packages for colors, advanced
%% formatting options, etc.
\usepackage{color}
\usepackage{microtype}
\usepackage{url}
\usepackage{cslatexquotes}
\usepackage{fancyvrb}
\usepackage[small,bf]{caption}
\usepackage[plainpages=false,pdfpagelabels,unicode]{hyperref}
\usepackage[all]{hypcap}

%% Fix long URLs in DVIs
\usepackage{ifpdf}

\ifpdf
\else
  \usepackage{breakurl}
\fi

%% Packages used to generate various lists
\usepackage{makeidx}
\makeindex

\usepackage[xindy]{glossaries}
\makeglossary

%% Use STAR and CIRCLE signs for nested
%% itemized lists
\renewcommand{\labelitemii}{$\star$}
\renewcommand{\labelitemiii}{$\circ$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------- Nastavení bakalářské práce (název, autor, atd.)  -----------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Title page information
\thesistitle{Implementace fulltextového vyhledávání v~systému správy požadavků}
\thesissubtitle{Bakalářská práce}
\thesisstudent{Jiří Holuša}
\thesiswoman{false} %% Important when using Slovak or Czech lang
\thesisfaculty{fi}  %% {fi, eco, law, sci, fsps, phil, ped, med, fss}
\thesislang{cs}     %% {en, sk, cs}
\thesisyear{Jaro 2014}
\thesisadvisor{Mgr. Filip Nguyen}

%% Beginning of the document
\begin{document}

%% Front page with a logo and basic thesis information
\FrontMatter
\ThesisTitlePage

%% Thesis declaration (required)
\begin{ThesisDeclaration}
  \DeclarationText
  \AdvisorName
\end{ThesisDeclaration}

%% Thanks (optional)
\begin{ThesisThanks}
TODO: poděkování
\end{ThesisThanks}

%% Abstract (required)
\begin{ThesisAbstract}
TODO: abstrakt
\end{ThesisAbstract}

%% Keywords (required)
\begin{ThesisKeyWords}
TODO: klíčová slova
\end{ThesisKeyWords}

%% Beginning of the thesis itself
\MainMatter

%% TOC (required)
\tableofcontents

%%% Words that shouldn't be hyphenated
\hyphenation{Hibernate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------- Vlastní text práce  -----------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Úvod}
Úvod

\chapter{Vyhledávání}
Tato kapitola stručně popisuje způsob vyhledávání skrze SQL v~nejčastějším datovém úložišti -- relačních databázích -- a uvádí jeho nedostatky. Poté se detailněji věnuje jednou z~možností jejich řešení, a to fulltextovým vyhledáváním. Uvádí nezbytnou 
teorii k~pochopení principů, jak fulltextové vyhledávání funguje.

\section{Vyhledávání v textu pomocí SQL}
\label{SQLsection}
Relační databáze poskytují vysoce výkonný přístup k~datům a široké možnosti pro jejich správu. Díky svých schopnostem se staly nejpoužívanější technologií pro datové uložiště. Pro komunikaci s~nimi se využívá jazyk SQL, který nabízí pro vyhledávání v~datech pouze dva způsoby: porovnání obsahu buňky a operátor \texttt{LIKE} \cite{MistrovstviMySQL}.

Porovnání obsahu buňky funguje na velice jednoduchém principu úplné shody obsahu. Na obrázku ref{SQLexample1} vidíme dotaz v~jazyce SQL, který vybere právě ty záznamy z~tabulky \texttt{People}, které mají hodnotu atributu \texttt{name} rovnou \uv{Bruce Banner}. 
\begin{figure}[!htbp]
\begin{lstlisting}[frame=single]
SELECT * FROM People WHERE name = 'Bruce Banner'
\end{lstlisting}
\caption{Jednoduché použití SQL pro vyhledávání pomocí úplné shody obsahu pole}
\label{SQLexample1}
\end{figure}

Nebudou vybrány žádné jiné záznamy, přestože by obsah atributu \texttt{name} byl např. \uv{Bruce Banners} či dokonce ani \uv{Bruce Banner } (přebytečná mezera na konci). Výhodou tohoto řešení je efektivita a jednoduchost -- jediná nutná operace je 
pouze porovnání dvou řetěžců, žádné dodatečné zpracování není potřeba. 

Trochu více sofistikovaným způsobem je operator \texttt{LIKE}, který umožňuje (v omezené míře) používat vyhledávání pomocí vzoru (\emph{pattern matching}). Podporovány jsou tzv. zástupné symboly (\emph{wildcards}), jež mohou mít v~tomto kontextu jiný význam než jen právě daný znak, např. symbol \% (procento) zastupuje libovolnou sekvenci znaků (třeba i žádnou) nebo znak \_ (podtržítko) libovolný, ale právě jeden znak. Obrázek \ref{SQLexample2} ukazuje příklad SQL dotazu, jenž nám vrátí všechny záznamy z~tabulky \texttt{People}, které jejich jméno končí na \uv{Banner}.
\begin{figure}[!htbp]
\begin{lstlisting}[frame=single]
SELECT * FROM People WHERE name LIKE '%Banner'
\end{lstlisting}
\caption{Použití SQL operátoru LIKE}
\label{SQLexample2}
\end{figure}

Nyní již dokázeme tímto dotazem získat jak lidi se jménem \uv{Bruce Banner}, tak i \uv{Richard Banner}. 

\section{Problémy vyhledávání pomocí SQL}
Předchozí kapitola představila základní způsoby vyhledávání pomocí SQL. Nyní se podíváme na případy, kde nám tyto způsoby nestačí nebo si s~danou situací nedokáží poradit buď vůbec, nebo pouze neefektivně.

Abychom si mohli tyto nedostatky demostrovat na příkladech, uvažujme existenci jednoduché relační databáze s~následujícím schématem (obrázek \ref{example_schema}).

\begin{figure}[htbp]
	\begin{center}
		\includegraphics{example_db_schema}
	\end{center}
	\caption{Datový model ukázkové databáze}	
	\label{example_schema}
\end{figure}

\subparagraph{Vyhledávání přes několik tabulek}
Představme si, že uživatel zadá do vyhledávacího políčka nějaký řetězec, na jehož základě očekává odpovídající výsledky. Vyvstává otázka, kde bychom měli zadanou frázi hledat. V~našem případě
pravděpodobně v~nadpisu, popisu, ve jméně a příjmení autora, tam všude by se mohly nacházet informace, které uživatel hledal. 

SQL nyní musí prohledat všechny zadané sloupce, které se však mohou nacházet v~různých tabulkách, což vede ke spojování tabulek. Možný příklad výsledného dotazu vidíme na obrázku \ref{SQLexample3}.
\begin{figure}[h!]
\begin{lstlisting}[frame=single]
SELECT *
FROM Kniha kniha 
LEFT JOIN kniha.autor autor 
WHERE kniha.titul = ? OR kniha.popis = ? OR 
autor.jmeno= ? OR autor.prijmeni= ?
\end{lstlisting}
\caption{SQL dotaz vyhledávající přes několik tabulek}
\label{SQLexample3}
\end{figure}

Je vidět, že i při relativně jednoduchém požadavku (vyhledáváme pouze ve čtyřech sloupcích) je výsledný dotaz poměrně složitý. Pokud bychom chtěli uživateli dát možnost využívat komplexnější
dotazy, je otázka generování odpovídajích SQL dotazů netriviální. Navíc uvažme, že musíme spojit více tabulek, což může vést k~problémům s~efektivitou \cite[s.~9]{HibernateSearchAction}.

\subparagraph{Vyhledávání jednotlivých slov}
Kapitola \ref{SQLsection} ukázala, že SQL dokáže vyhledat v~jednotlivých sloupcích přesně zadanou frázi. Je ovšem velice nepravděpodobné, že sloupce v~databázi budou obsahovat přesně stejnou danou frázi, 
hledání jednotlivých slov by nám velice zvýšilo pravděpodobnost nálezu \cite[s.~9]{HibernateSearchAction}. SQL však žádnou takovou funkcionalitu na dělení vět neposkytuje, je tedy nutné si větu předpřipravit explicitně (tj. rozdělit na slova),
a poté spouštět vyhledávácí dotaz pro každé slovo zvlášť. Následně výsledky nějakým způsobem sloučit. Takové řešení však nebude dostatečně efektivní \cite[s.~10]{HibernateSearchAction}. 

\subparagraph{Filtrace šumu}
Některá slova ve větách nenesou vzhledem k~vyhledávání žádnou informační hodnotu, např. spojky či předložky či ještě lepším příkladem mohou být anglické neurčité členy. Taková slova se nazývají šum (\emph{noise}). Dále se pak některá slova v~určitém kontextu
šumem stávají, např. slovo \uv{kniha} v~našem internetovém knihkupectví \cite[s.~9]{HibernateSearchAction}. Jelikož šum nenese žádnou informační hodnotu, měl by být při hledání ignorován. SQL nám opět neposkytuje žádný prostředek k~řešení tohoto problému.

\subparagraph{Vyhledávání příbuzných slov}
Je velice žádoucí, abychom se při vyhledávání mohli zaměřit pouze na význam hledaného slova, nikoliv na jeho tvar. Nemělo by záležet na tom, zda hledáme frázi \uv{fulltextové hledání} nebo \uv{fulltextových vyhledávání}, význam těchto frázi
je stejný. Jinak řečeno, vyhledávání by mělo brát v~potaz i slova odvozená, se stejným kořenem. Ještě pokročilejším požadavkem by mohla být možnost zaměňovat slova s~jejich synonymy, např. \uv{upravit} a \uv{editovat} \cite[s.~10]{HibernateSearchAction}.

SQL nám nenabízí možnost k~řešení těchto požadavků, klíčem by mohl být slovník příbuzných slov a synonym a pokusit se vyhledávat i podle něj. Takové řešení však přináší nezanedbatelné množství práce, nehledě na nutnost existence takového slovníku.

\subparagraph{Oprava překlepů}
Uživatel je člověk a jako člověk je omylný a dělá chyby. Vyhledávání by to mělo brát v~potaz a snažit se tyto překlepy opravit či uhodnout, co měl uživatel na mysli. Když v~našem internetovém knihkupectví uživatel hledá knihu \uv{Fulltextové vyhledávání} a
omylem zadá do vyhledávacího pole \uv{Fulltetové vyhledávání}, je žádoucí, aby i přes tento překlep knihu našel \cite[s.~10]{HibernateSearchAction}.

\subparagraph{Relevance}
Pravděpodobně největším problémem v~SQL je absence jakéhokoliv mechanismu pro určení míry shody (\emph{relevance}) záznamu se zadaným dotazem \cite[s.~10]{HibernateSearchAction}. Předpokládejme, že v~našem knihkupectví napsal autor \uv{John Smith} 100 knih, jednu o~fulltextovém vyhledávání a zbytek naprosto nesouvisející s~informatikou. Dále několik dalších autorů rovněž napsalo publikace na téma fulltextového vyhledávání.

Pokud jako uživatel víme, že je autorem John Smith a kniha je o~fulltextovém vyhledávání, očekáváme, že na vyhledávací dotaz \uv{John Smith fulltextové vyhledávání} obdržíme nejdříve právě chtěnou knihu, a poté teprve knihy ostatní od našeho autora či
další knihy o~fulltextovém vyhledávání, jelikož naše kniha \uv{nejvíce} odpovídala položenému dotazu.

\section{Fulltextové vyhledávání}
Předchozí kapitola demostrovala, jaké problémy má vyhledávání pomocí SQL. Nyní si bude představeno možné řešení -- fulltextové vyhledávání. 

\subsection{Úvod do fulltextového vyhledávání}
\label{UvodDoFulltextu}
Fulltextové vyhledávání (někdy také \emph{fulltext} nebo \emph{full-text}) je speciální způsob vyhledávání informací v~textu. Vyhledávání probíhá porovnáváním s~každým slovem v~hledaném textu. Jelikož počet slov v~textu může teoreticky neomezený a jelikož je 
nutné, aby vyhledávání bylo co nejrychlejší, funguje fulltextové vyhledávání ve dvou fázích: \emph{indexace} a \emph{hledání} \cite[s.~11]{HibernateSearchAction}. 

\subsection{Indexace}
Indexace je hlavním krokem ve fulltextovém vyhledávání. Jedná se o~proces předpřipravení vstupních dat, jejich přeměnu na co nejvíce efektivní datovou strukturu, aby se v~ní dalo snadno a rychle vyhledávat. Této datové struktuře, která je výstupem indexace, se říká \emph{index} \cite[s.~11]{LuceneAction}. 

Index si lze představit jako datovou struktu umožňující přímý přístup ke slovům v~něm obsažených. Základním úkolem je rozdělit text do slov a pomocí přímého přístupu umožnit velice efektivně zjistit, kde se dané slovo vyskytuje. Toho je typicky (např. v~Apache Lucene) dosaženo \emph{invertovaným indexem} \cite[s.~35]{LuceneAction}. %% Invertovaný index je datová struktura, která ukládá pro svůj klíč (např. nějaký text) místo, kde se nachází.

Pouhým rozdělením do slov však možnosti předpřipravení textu nekončí a může být zapojena složitá analýza. V~praxi (např. v~Apache Lucene \cite[s.~35]{LuceneAction}) je celý text předáván analyzátoru, který může index libovolně budovat, a tím ho lépe připravit na nadcházející dotazování, a umožnit mu odpovídat na složitější dotazy. Typickým příkladem možné analýzy je úprava podstatných jmen do základního tvaru (např. z~množného čísla na jednotné), přidání synonym do indexu či získávání statistiky o~četnosti výskytu daného slova.

\subsection{Hledání}
Samotné vyhledávání v~textu je ve fulltextovém vyhledávání realizováno nikoliv nad textem samotným, ale nad předpřipraveným indexem z~procesu indexace \cite[s.~15]{HibernateSearchAction}. Vyhledávácí nástroj tedy může využít doplňkových informací o~textu, které dokáží vyhledávání zrychlit. Jakým způsobem je index budován a jak se nad ním následně vyhledává, záleží pak již na konkrétní technologii.

\chapter{Dostupné technologie}
Pro platformu Java existuje řada dostupných volně širitelných vyhledávacích technologií. Nyní si představíme tři z~nich: \emph{Apache Lucene}, \emph{Hibernate Search} a \emph{Elasticsearch}.

\section{Apache Lucene}
Apache Lucene je vysoce výkonná, škálovatelná, volně širitelná vyhledávací knihovna napsána v~jazyce Java \cite[s.~6]{LuceneAction}. Autorem projektu, který vznikl v~roce 1997, je Doug Cutting. Zajímavostí je, že jméno Lucene bylo vybráno podle druhé jména manželky autora \cite[s.~6]{LuceneAction}. V~roce 2000 zveřejnil Lucene na stránkách serveru SourceForge.com a uvolnil ji tak zdarma pro komunitu. O~rok později byla adoptována organizací \emph{Apache Software Foundation}. Od té doby se knihovna neustále vyvíjela a v~dubnu roku 2014 je aktuálně dostupná ve své nejnovější verzi 4.7.1 \cite[s.~6]{LuceneAction}.

Již několik let je Lucene nejpopulárnější vyhledávací technologií zdarma. Díky své popularitě se však dočkala i přepsání do jiných jazyků než je Java jako například Perl, Python, Ruby, C/C++, PHP a C\# (.NET) \cite[s.~3]{LuceneAction}. Projekt je stále aktivně vyvíjen s~širokou komunitní základnou.

Apache Lucene není hotová vyhledávací aplikace, je to knihovna, nástroj, poskytující všechny potřebné prostředky, aby mohla být taková aplikace pro vyhledávání naprogramována. Nabízí rozhraní pro vytváření, úpravu indexu, zpracování dat před indexací a tvorbu, úpravu dotazů a mnoho dalšího. O~zbytek úkonů se musí programátor postarat sám, z~čehož vyplývají hlavní výhoda (robustnost, univerzálnost použití), ale také hlavní nevýhoda (složitost nasazení) \cite[s.~7]{LuceneAction}.

Používání Apache Lucene je poměrně náročné, což vychází z~její univerzálnosti \cite{ElasticsearchDefinitiveGuide}  -- uživatel (programátor) má mnoho možností, jak výslednou vyhledávací aplikaci nakonfigurovat, a tím i vyladit. Kvůli této složitosti začaly vznikat další technologie, které staví na Apache Lucene, snaží se schovat podrobná, a tedy i méně často používaná, nastavení do pozadí a umožnit tak vývojáři se v~technologii rychle zorientovat se zachováním původní síly Apache Lucene. Takových technologií existuje více (Apache Solr, Hibernate Search, Elasticsearch a další) a je dobré při jejich používání vědět, jak funguje Apache Lucene na nižší úrovni, neboť tyto technologie ji přímo využívají. Z toho důvodu si podrobněji představíme architekturu Apache Lucene, abychom poznali její sílu a možnosti.

\subsection{Architektura}
Abychom pochopili, jak Apache Lucene funguje, představíme si zběžně její architekturu. Níže následuje výčet základních tříd, které se podílejí na procesu indexace \cite[s.~26]{LuceneAction}:
\begin{itemize}
	\item \texttt{IndexWriter}
	\item \texttt{Directory}
	\item \texttt{Analyzer}
	\item \texttt{Document}
	\item \texttt{Field}
\end{itemize}

\begin{figure}[htp]
	\begin{center}
		\includegraphics[width=340pt]{lucene_architecture}
	\end{center}
	\caption{Architektura indexační části Apache Lucene, převzato z~\cite[s.~26]{LuceneAction}}	
\end{figure}

Třída \texttt{IndexWriter} je vstupní bod indexace. Je zodpovědná za vytváření nového indexu a přidávání dokumentů do indexů existujících. Neslouží k~vyhledávání ani modifikaci indexu. \texttt{IndexWriter} musí znát umístění, kam má svůj index uložit a k~tomu slouží \texttt{Directory}. 

\texttt{Directory} je abstraktní třída reprezentující fyzické umístění indexu.  

Předtím než je text indexován, je předán analyzéru, implementaci abstraktní třídy \texttt{Analyzer}. Analyzér je zodpovědný za extrakci \emph{tokenů} -- jednotek, které následně budou skutečně uloženy do indexu \cite[s.~116]{LuceneAction} -- a eliminaci všeho ostatního. Analyzér je patrně nejdůležitější komponenta indexace, rozhoduje, které tokeny budou uloženy a dokáže je libovolně modifikovat. Apache Lucene obsahuje již některé praktické implementace třídy \texttt{Analyzer}, které jsou nejběžnější. Některé z~nich  se například zabývají odstraněním šumu z~textu, další převedením všech písmen na malá apod. Proces analýzy podrobněji rozebíráme v~další kapitole, neboť je to klíčová vlastnost Apache Lucene, kterou dědí i ostatní technologie na ní postavené.

\texttt{Document} reprezentuje kolekci polí (\emph{fields}), je to kontejner pro objekty \texttt{Field}, které nesou textová data. 

\texttt{Field} je základní jednotka, která obsahuje vlastní indexovaný text.

Jak je uvedeno v kapitole \ref{UvodDoFulltextu}, fulltextové vyhledávání má dvě části -- indexaci a vyhledávání. Protože však technologie postavené na Apache Lucene poskytují své vlastní vyhledávací API, a tím skrývájí vyhledávání v~Apache Lucene úplně, nebudeme se detaily architektury vyhledávání v~Apache Lucene dále zabývat. 

\subsection{Indexace}
Předchozí kapitola stručně popisuje architekturu indexační části Apache Lucene. Nyní si vysvětlíme, jak spolu jednotlivé části spolupracují.

Základní jednotkou indexu Apache Lucene jsou \emph{dokumenty} (\emph{directories}) a \emph{pole} (\emph{fields}) \cite[s.~32]{LuceneAction}. Dokument je kolekcí polí, které pak obsahují \uv{skutečný} obsah. Každé pole má své jméno, textovou nebo binární hodnotu a seznam operací, které popisují, co má Apache Lucene dělat s~hodnotou pole při vytváření indexu. Abychom mohli indexovat naše uživatelská data (položky z~databáze, PDF dokumenty, HTML stránky apod.), je potřeba je převést do formátu Apache Lucene dokumentu. Apache Lucene nemá ponětí o~sématice obsahu, který indexuje. Převedením struktury našeho obsahu do struktury Lucene dokumentů, do dvojic klíč:hodnota, se zabývá \emph{denormalizace}.

\subparagraph{Denormalizace}
Denormalizace je proces převedení libovolné struktury dat do jednoduchého formátu klíč:hodnota \cite[s.~34]{LuceneAction}. Například v~databázi jsou jednotlivé záznamy spojovány cizími klíči mezi různými tabulkami, vzniká mezi nimi vztah, jednotlivé záznamy se na sebe odkazují. V~dokumentech Apache Lucene však žádná možnost odkazu či spojení není, jediný akceptovaný formát je, jak jsme si řekli, klíč:hodnota. Programátor musí vyřešit problém, jak data, ve kterých chce vyhledávat, denormalizuje. Apache Lucene nechává tuto část zcela na programátorovi, na rozdíl od na ní postavených technologiích jako např. Hibernate Search.

Jednou z~dalších důležitých věcí, které je potřeba vědět o~Apache Lucene dokumentech, je absence jakéhokoliv pevného schématu jako např. u~databází. Tato vlastnost se někdy označuje jako \emph{flexibilní schéma} \cite[s.~34]{LuceneAction}. Umožňuje nám například iterativně budovat index, protože nově nahraný index může být naprosto rozdílný, obsahovat jiná pole, od předchozího. Rovněž můžeme do jednoho dokumentu uložit indexy reprezentující zcela jiné entity.

\subsection{Analýza}
V předchozích kapitolách jsou uvedeny fundamentální základy, na kterých Apache Lucene staví indexy, nyní se blíže podíváme nejdůležitější část indexačního procesu~--~analýzu.

Předpokládejme, že vstupní data máme již denormalizována do dokumentů, které jsou naplněny poli. Analýza v~Apache Lucene je proces převedení textových polí do základní indexované podoby -- do termů \cite[s.~28]{LuceneAction}. Analyzérem nazýváme komponentu, která zajišťuje analýzu. Ukažme si několik typických příkladů, co analyzéry dělají  \cite[s.~110]{LuceneAction}:

\begin{itemize}
	\item extrakce slov
	\item zahození interpunkce
	\item převod na malá písmena (\emph{normalizace})
	\item redukce šumu
	\item převod slova na jeho kořen (\emph{stemming})
	\item převod slova na základní tvar (\emph{lemmatizace})
	\item a další 
\end{itemize}

Samozřejmě je možné naprogramovat vlastní analyzér, některé úkony jsou však natolik běžné (jako například výše uvedené), že Apache Lucene přichází s~několika zabudovanými analyzéry. Analyzéry pro svou funkčnost využívají dva další typy komponent: \emph{tokenizéry} (potomky třídy \texttt{Tokenizer}) a \emph{filtry} (potomky třídy \texttt{TokenFilter})  \cite[s.~115]{LuceneAction}. Obě dědí od abstraktní třídy \texttt{TokenStream}, zabývají se však rozdílnou částí zpracování vstupu. Tokenizér čte vstup a vytváří tokeny. Filtr bere jako vstup tokeny a na jejich základě vrátí nově vytvořený seznam tokenů. Tento seznam může vzniknout přidáním nových tokenů, úpravou existujících či odstraněním některých z~nich. 

Typické využití, kterého se drží i zabudované analyzéry, vypadá následovně. Analyzéru je předán vstup. Ten je rozdělen na tokeny pomocí jednoho tokenizéru. Následně jsou tokeny předány jednomu či více filtrům, čímž vznikne finální kolekce tokenů, která je předána jako výsledek analýzy (obrázek \ref{AnalysisLucene}).

\begin{figure}[htp]
	\begin{center}
		\includegraphics[width=340pt]{lucene_analysis}
	\end{center}
	\caption{Použití tokenizéru a filtrů, převzato z~ \cite[s.~117]{LuceneAction}}	
	\label{AnalysisLucene}
\end{figure}

Uveďme si příklady zabudovaných tokenizérů  \cite[s.~118]{LuceneAction}:

\begin{itemize}
	\item \texttt{WhitespaceTokenizer} - nový token je ohraničen bílými znaky
	\item \texttt{KeywordTokenizer} - předá celý vstup jako jeden token
	\item \texttt{LowerCaseTokenizer} - nový token je ohraničen jinými znaky než písmeny
	\item \texttt{StandardTokenizer} - pokročilý tokenizér založený na sofistikovaných gramatických pravidel, dokáže rozpoznat např. e-mailové adresy a předat je jako jediný token
\end{itemize}

Představme si rovněž i několik základních filtrů  \cite[s.~118]{LuceneAction}
\begin{itemize}
	\item \texttt{LowerCaseFilter} - převede token na malá písmena
	\item \texttt{StopFilter} - odstraní tokeny, které se nacházejí v~předaném seznamu
	\item \texttt{PorterStemFilter} - převádí tokeny na jejich kořen (\emph{stemming})
	\item \texttt{LengthFilter} - akceptuje tokeny, jejichž délka spadá do určitého rozsahu
	\item \texttt{StandardFilter} - navržen pro spolupráci s~tokenizérem StandardTokenizer, odstraňuje tečky z~akronymů a \uv{'s} (apostrof následovaný písmenem s)
\end{itemize}

\begin{figure}[!htbp]
\begin{lstlisting}[frame=single]
WhitespaceAnalyzer:
[The] [quick] [brown] [fox] [jumped]
[over] [the] [lazy] [dog]

SimpleAnalyzer:
[the] [quick] [brown] [fox] [jumped] 
[over] [the] [lazy] [dog]

StopAnalyzer:
[quick] [brown] [fox] [jumped] [over] [lazy] [dog]

StandardAnalyzer:
[quick] [brown] [fox] [jumped] [over] [lazy] [dog]
\end{lstlisting}
\caption{Použití zabudovaných analyzérů pro větu \emph{\uv{The quick brown fox jumped over the lazy dog}}}
\label{AnalysisExample1}
\end{figure}

Aby byl výčet kompletní, následuje přehled zabudovaných analyzérů. Zabudované analyzéry jsou v~podstatě kombinací tokenizérů a filtrů, z~čehož je následně jasná jejich funkce  \cite[s.~112]{LuceneAction}.

\begin{itemize}
	\item \texttt{WhitespaceAnalyzer} - dělí text na tokeny pomocí tokenizéru \\ \texttt{WhitespaceTokenizer}
	\item \texttt{SimpleAnalyzer} - zpracovává vstup pomocí tokenizéru \\ \texttt{LowerCaseTokenizer}
	\item \texttt{StopAnalyzer} - kombinace tokenizéru \texttt{LowerCaseTokenizer} a filtru \texttt{StopFilter}, kterému je předán seznam často se vyskytujících nevýznamových slov v~angličtině (členy \emph{a}, \emph{an}, \emph{the}, apod.)
	\item \texttt{StandardAnalyzer} - nejpropracovanější zabudovaný analyzér, využívá \texttt{LowerCaseTokenizer}, \texttt{StopFilter}, navíc však přidává i propracovanou logiku, která dokáže např. rozeznat e-mailové adresy, názvy společností atd.
\end{itemize}

Popis analýzy zakončeme ukázkami, jaké tokeny jednotlivé zabudované analyzéry vytvoří ze dvou anglických vět \emph{\uv{The quick brown fox jumped over the lazy dog}} (obrázek \ref{AnalysisExample1}) a \emph{\uv{XY\&Z Corporation - xyz@example.com}} (obrázek \ref{AnalysisExample2}).

\begin{figure}[!htbp]
\begin{lstlisting}[frame=single]
WhitespaceAnalyzer:
[XY&Z] [Corporation] [-] [xyz@example.com]

SimpleAnalyzer:
[xy] [z] [corporation] [xyz] [example] [com]

StopAnalyzer:
[xy] [z] [corporation] [xyz] [example] [com]

StandardAnalyzer:
[xy&z] [corporation] [xyz@example.com]
\end{lstlisting}
\caption{Použití zabudovaných analyzérů pro větu \emph{\uv{XY\&Z Corporation - xyz@example.com}}}
\label{AnalysisExample2}
\end{figure}

\section{Hibernate Search}
Po rozmachu technologie \emph{objektově relačního mapování} (ORM, \emph{Object-Relational Mapping}) na platformě Java a její nejznámější implementace Hibernate Core \cite[s.~29]{HibernateSearchAction} bylo nutné rovněž dát tomuto nástroji možnosti fulltextového vyhledávání, o~což se postarala právě knihovna Hibernate Search. Hibernate Search je volně šiřitelná knihovna napsaná Emmanuelem Bernardem, která doplňuje Hibernate Core o~možnosti fulltextového vyhledávání pomocí kombinace s~Apache Lucene  \cite[s.~29]{HibernateSearchAction}. Hibernate Search se snaží zabalit komplexnost Apache Lucene do jednoduší podoby a integrovat funkčnost do Hibernate ORM. S minimálním úsilím za nás řeší převod objektového datového modelu do podoby přijatelné pro Apache Lucene, čímž výrazně usnadňuje její použití.

Obrázek \ref{ExampleHibernateSearch} demonstruje, jak snadno lze s~využitím Hibernate Search zpřístupnit entitu pro fulltextové vyhledávání. Entita musí být označena anotací \texttt{@Indexed}  \cite[s.~38]{HibernateSearchAction}. Dále přidáme anotaci \texttt{@DocumentId} k~primárnímu klíči, a poté označíme atributy, podle kterých chceme vyhledávat anotací \texttt{@Field}  \cite[s.~38]{HibernateSearchAction}. V~momentě uložení entity za nás Hibernate Search vyřeší přidání uvedených atributů do indexu, tedy denormalizuje entitu. Jelikož je to však pod povrchem stále Apache Lucene, máme k~dispozici všechny možnosti, které nám nabízí, nyní v~přístupnější formě.

\begin{figure}[!htbp]
\begin{lstlisting}[frame=single]
@Entity
@Indexed
public class Person {

	@Id 	@GeneratedValue
	@DocumentId
	private Long id;

	@Field
	private String firstName;

	@Field 
	private String lastName;
}
	\end{lstlisting}
	\caption{Zpřístupnění entity pro vyhledávání v~Hibernate Search}
	\label{ExampleHibernateSearch}
\end{figure}

Integrace s~Hibernate Core za nás elegantně řeší jeden podstatný problém, který máme s~použitím čistě Apache Lucene - synchronizaci fulltextového indexu a obsahu databáze. Jsou to v~zásadě dvě zcela oddělená datová úložiště, která spolu úzce souvisí. Pokud používáme přímo Apache Lucene, je nutné se po manipulaci s~objektem v~databázi explicitně postarat o~úpravu příslušného indexu, což je pro programátora práce navíc. Oproti tomu Hibernate Search je navázán na události Hibernate Core, tudíž při úpravě objektu v~databázi je automaticky spuštěn proces aktualizace indexu, aby spolu byla data v~databázi a fulltextovém indexu synchronizována  \cite[s.~24]{HibernateSearchAction}. 

\section{Elasticsearch}
\label{ElasticsearchChapter}
Elasticsearch je distribuovaný vyhledávácí a analytický nástroj v~reálném čase \cite{ElasticsearchDefinitiveGuide}. Historie této technologie se začala psát v~roce 2004, kdy Shay Banon vytvořil \emph{Compass}. Postupným vývojem a změnou požadavků však dospěl k~názoru, že aby se mohl Compass stát distribuovanou technologií, bylo by zapotřebí ho značnou část přepsat. Rozhodl se proto naprogramovat zcela nový nástroj, který měl být již od počátku distribuovaný. První verze Elasticsearch byla vydána v~únoru 2010 \cite{ElasticsearchWiki}.

Elasticsearch je rovněž postaven na dříve představené technologii Apache Lucene, k~níž však přidává další klíčové vlastnosti. Řeč je zejména o~celé architektuře. Elasticsearch není na rozdíl od Apache Lucene knihovnou, Elasticsearch tvoří samostatný distribuovaný systém serverů, které na pozadí používají Apache Lucene, ovšem skrývají její složitost a poskytují služby v~mnohem jednodušším uživatelském API. Velký důraz je kladen právě na distribuovanost celého systému, proto je Elasticsearch vysoce škálovatelný, schopný vytvořit klastr několika stovek serverů, a tím zajistit vysoký výkon i při několika petabajtech dat, což patří mezi hlavní přidanou hodnotu navrch k~Apache Lucene \cite{ElasticsearchDefinitiveGuide}.

Základním způsobem komunikace se serverem Elasticsearch je REST (\emph{Representational State Transfer}) API posílající JSON (\emph{JavaScript Object Notation}) objekty. Tím získáváme naprostou nezávislost na programovacím jazyku, komunikace může probíhat přímo i z~příkazové řádky. Do některých jazyků, jako například Java, PHP, Python, však byli napsáni klienti, kteří umožňují komunikaci přímo z~onoho jazyka. Stejně jako Hibernate Search je tedy i Elasticsearch zaobalená knihovna Apache Lucene s~několika přidanými hodnotami \cite{ElasticsearchDefinitiveGuide}. 

\chapter{Návrh a implementace}
Předchozí kapitoly představily fulltextové vyhledávání a dostupné technologie pro jeho implementaci na platformě Java. Následující text se věnuje skutečné implementaci fulltextového vyhledávání v~systému správy požadavků eShoe. 

\section{Specifikace požadavků}
\label{SpecifikacePozadavku}
Hlavním úkolem je vytvořit funkční fulltextové vyhledávání v systému eShoe. Tento požadavek lze rodělit do několika menších částí shrnutých v následujících bodech:
\begin{itemize}
	\item \textbf{Provedení indexace:} \\ 
		 Při modifikaci entity v databázi musí být entita rovněž vhodně uložena do indexu pro fulltextové vyhledávání, aby mohla být následně vyhledávána.

	\item \textbf{Vyhledávání nad indexovanými daty:} \\
		Na základě dotazu uživatele musí být index prohledán a vráceny relevantní výsledky.

	\item \textbf{Zobrazení výsledků:} \\
		Vytvořit jednoduché uživatelské rozhraní, které umožní pokládat dotazy a zároveň zobrazí výsledky.		
\end{itemize}

Protože musíme vybrat vhodné vlastnosti entit, které mají být indexovány, je nutné specifikovat dotazy, které by uživatel mohl chtít položit a systém by na ně měl umět vrátit požadovanou odpověď. Kromě obecného zadání fráze, na jejímž základě mají být vráceny relevantní požadavky, je žadoucí umožnit uživateli zadat vlastnosti, které musí požadavek splňovat a všechny nevyhovující odfiltrovat, například vyhledat všechny požadavky na dotaz \uv{NullPointerException}, ale pouze ty přiřazené k projektu \uv{Infinispan}. Následuje výčet vlastností, které může uživatel explicitně zadat, a podle kterých systém umožní požadavky filtrovat:
\begin{itemize}
	\item projekt, ke kterému je požadavek přiřazen
	\item status, v němž se požadavek nachází
	\item typ požadavku
	\item datum vytvoření
	\item datum poslední modifikace
	\item jméno uživatele, který požadavek vytvořil
	\item jméno uživatele, kterému přiřazeno řešení požadavku
	\item prioritu požadavku	 
\end{itemize}

Dalším bodem ze zadání je vytvořit mechanismus pro import dat z již existujícího systému správy požadavků do systému eShoe. Systém musí být schopen na základě předaného seznamu požadavků z onoho existujícícho systému namapovat požadavky na datový model systému eShoe, uložit je do databáze a fulltextového indexu.

\section{Návrh}
V předchozí sekci je uvedena specifikace, kterou musí implementace splňovat. Následující text popisuje fázi návrhu, jak budou jednotlivé body specifikace vyřešeny. 

Pro implementaci fulltextového vyhledávání byla zvolena technologie Elasticsearch z následujících důvodů. Jedná se o relativně mladou technologii, která je však postavena na osvědčené Apache Lucene, s aktivní komunitní základnou a stálým vývojem. Je používána např. serverem GitHub \cite{ElasticsearchDefinitiveGuide}, z čehož lze usuzovat, že poskytne i dostatečný výkon. Kromě toho nám Elasticsearch přišel subjektivně nejvíce elegantní.

\subparagraph{Indexace}
První z~problémů, který je potřeba vyřešit, je zvládnutí procesu indexace, tedy denormalizaci entit do formátu, který se dá přímo předat Elasticsearch serveru. Elasticsearch příjímá JSON objekty (viz. \ref{ElasticsearchChapter}, musíme tedy entity převést právě do JSON formátu. Jedním z~prvních možných řešení je prostá manuální tvorba indexu z~entity pomocí \emph{get} metod, to znamená pro každou třídu vytvořit mechanismus, který v~předem daném pořadí předem dané atributy získá a vytvoří z~nich JSON objekt.

Nevýhoda tohoto řešení je zjevná -- nulová flexibilita. Při každé úpravě entity je nutné dopsat odpovídající mechanismus, který upravený atribut denormalizuje. Navíc je toto řešení udělané přesně na míru tomuto projektu, resp. přesně danému datovému modelu, tudíž není znovupoužitelné do budoucna. Výhoda je ovšem rovněž zřejmá -- jednoduchost. K naprogramování takového mechanismu není potřeba víc než základní znalost jazyka Java. Z důvodu programování kódu, který by byl použitelný pouze v jednom projektu a je poměrně neelegantní bylo toto řešení zavrhnuto a hledali jsme alternativní přístup.

Po prostudování dokumentace pro Elasticsearch jsme zjistili, že v součané době není pro jazyk Java naprogramován žádný nástroj, jenž by usnadnil denormalizaci objektů, jako je tomu např. v Hibernate Search (viz. \ref{ExampleHibernateSearch}). Bylo proto rozhodnuto, že podobný mechanismus naprogramujeme první a poskytneme podobnou funkcionalitu i pro Elasticsearch.

Základní myšlenkou je použití anotací, což je zárukou vysoké elegance a jednoduchosti použití. Jakmile jsou atributy entity označeny anotacemi, entity se předá správci indexu, který entitu denormalizuje, připojí se skrze zvoleného klienta k serveru Elasticsearch a uloží nově vytvořený dokument do indexu opět na základě zadaných parametrů u anotací. Tento nově vzniklý projekt byl pojmenován \emph{Elasticsearch-Annotations}.

Protože při vývoji projektu eShoe nebyla potřeba servisní vrstva aplikace, tak zcela chybí. Nyní je však nutné navázat operace změny indexu na změny v databázi a servisní vrstva by byla místem, kde by se to dalo realizovat. Proto byla servisní vrstva nově vytvořena. Vyřešení indexace pak spočívá v označení entit anotacemi a zavoláním správce indexu na servisní vrstvě. 

Jelikož je proces denormalizace zcela oddělen do projektu Elasticsearch-Annotations a pro zakomponování indexačního mechanismu do datového modelu eShoe vyžaduje minimální úpravy, je toto řešení elegantní a vysoce znovupoužitelné. Bylo proto rozhodnuto se ubírat tímto směrem a tento návrh implementovat.

\subparagraph{Vyhledávání}
Jakmile máme data zaindexována, můžeme přistoupit k vlastnímu vyhledávání. Jedná se hlavně o způsob tvorby dotazu pro Elasticsearch server. Kapitola \ref{SpecifikacePozadavku} uvádí systémem podporovné typy dotazů, který je několik a patrně v budoucnu přibydou další. Typů dotazů je několik a poměrně různorodých, proto je potřeba vymyslet robustní způsob zadávání dotazů, který by se mohl dále rozšiřovat.

Základním způsobem je tvorba dotazu přes uživatelské rozhraní, na pozadí by se postupně budoval objekt reprezentující dotaz pro Elasticsearch, a poté se vyhodnotil. Musíme však brát v potaz uživatele, kteří by chtěli vyhledávání používat skrze nějaký automatizovaný mechanismus, nikoliv ručně klikáním na komponenty v GUI. Pro ty by automatizace tvorby dotazu nebyla jednoduchá.

Další možností je vytvořit vlastní dotazovací jazyk, jako má např. Attlasian JIRA. Uživatel by dostal možnost vytvářet dotazy buď klikáním v GUI, nebo by rovnou mohl napsat dotaz v dotazovacím jazyce. Proces automatizace se tím pádem velice zjednodušuje na předání vytvořeného dotazu v dotazovacím jazyce do parametru stránky. Pro některé uživatele je dokonce pohodlnější napsat dotaz rovnou v dotazovacím jazyce, pokud je dostatečně jednoduchý. Pro poskytnutí maximálně flexibility se přikláníme k tomuto řešení.

Vyhledávání bude probíhat na základě dotazu vytvořeném ve vlastním dotazovacím jazyce. Uživatelské rozhraní bude sloužit jako tvůrce oněch dotazů umožňující rovněž zadávání dotazu přímo. Dotazovací jazyk bude mít následující vlastnosti:
\begin{itemize}
	\item zadat text, který se má použít jako fráze pro fulltextového vyhledávání
	\item zadat filtr na vlastnost entity na přesnou shodu jedné položky
	\item určit filtr na vlastnost entity na shodu s některou ze seznamu předaných hodnot
	\item předchozí body libovolně kombinovat
\end{itemize}

Uvedený jazyk by měl být co nejjednoduší a mít intuitivní syntaxi. Obrázky \ref{QueryLanguageExample1}, \ref{QueryLanguageExample2} a \ref{QueryLanguageExample3} uvádí příklady, jak bude výsledný dotazovací jazyk vypadat.

\begin{figure}[htbp]
\begin{lstlisting}[frame=single]
text ~ "NullPointerException"
AND project = "Infinispan"
\end{lstlisting}
\caption{Vyhledání fráze \uv{NullPointerException} pouze u projektu \uv{Infinispan}}
\label{QueryLanguageExample1}
\end{figure}

\begin{figure}[htbp]
\begin{lstlisting}[frame=single]
text ~ "NullPointerException" 
AND status IN ("Unresolved", "Open")
\end{lstlisting}
\caption{Vyhledání fráze \uv{NullPointerException} u požadavků se statusem \uv{Unresolved} nebo \uv{Open}}
\label{QueryLanguageExample2}
\end{figure}

\begin{figure}[htbp]
\begin{lstlisting}[frame=single]
text ~ "NullPointerException" 
AND project = "Infinispan" 
AND status IN ("Unresolved", "Open")
\end{lstlisting}
\caption{Kombinace dotazů \ref{QueryLanguageExample1} a \ref{QueryLanguageExample2}}
\label{QueryLanguageExample3}
\end{figure}

\subparagraph{Uživatelské rozhraní}
Přestože zadání práce ani specifikace uživatelské rozhraní nevyžaduje, rozhodli jsme se jej přesto zahrnout, aby byla demostrace výsledků snažší a aby byl poskytnut prototyp pro další rozvíjení. Součástí grafického rozhraní budou textová pole a tři pole pro vybrání ze seznamu. Textová pole budou použita pro zadání fráze pro vyhledání a ohraničení časového úseku, kdy byl požadavek vytvořen. Pole s předem daným seznamem prvků budou sloužit ke zvolení typů požadavků, jejich statusů a zvolení projektů, ke kterým mají být požadavky přiřazeny. Je zřejmé, že tato pole musí podporovat výběr více možností najednou.

Dále grafické rozhraní umožní jednoduchý výpis nalezených požadavků a umožní přejít přímo na stránku s detailními informacemi o požadavku.

\subparagraph{Import dat}
Součástí specifikace je i import dat z již existujícího systému pro správu požadavků. Pro tento účel byl vybrán systém Red Hat Bugzilla. Red Hat Bugzilla nabízí REST API pro komunikaci se systémem. Skrze tento přístupový bod musí být systém schopen získat požadavky, jejichž ID bude předáno v konfiguračním souboru importovacího mechanismu. Následně bude požadavek ze systému Red Hat Bugzilla převeden na odpovídající entity v datovém modelu eShoe a uložen do databáze, respektive fulltextového indexu.

\section{Použité technologie}
Při implementaci bylo použito několik dalších technologií (kromě Elasticsearch), některým z nich se věnuje následující text. Při výběru byl kladen důraz nejen na jejich přínosnou hodnotu, ale také na jejich volnou šiřitelnost, neboť celý projekt eShoe se snaží být rovněž volně širitelný.

\subsection{Maven}
Pro usnadnění procesu vývoje se často používají různé nástroje pro automazaci sestavení a překladu zdrojového kódu. Apache Maven je jedním z předních nástrojů pro správu projektů na platformě Java.

\subsection{Git}
Git je volně širitelný systém pro distribuovanou správu verzí. 
\subsection{Jackson}
\subsection{ANTLR}

\section{Implementace}

\subsection{Indexace}

\subsection{Vyhledávání}

\subsection{Uživatelské rozhraní}

\subsection{Import dat}

\chapter{Závěr}
Závěr


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%------- Konec vlastního textu práce  -----------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Lists of tables and figures, glossary, etc.
\printindex
%\printglossary
%\listoffigures
%\listoftables

%% Bibliography from references.bib
\begingroup
\def\tmpchapter{0}
\renewcommand{\chaptername}{}
\renewcommand{\thechapter}{}
\addtocontents{toc}{\setcounter{tocdepth}{-1}}
\chapter{Literatura}
\renewcommand{\chapter}[2]{}% for other classes

\begin{thebibliography}{1}

\bibitem{MistrovstviMySQL}
KOFLER, Michael. \textit{Mistrovství v~MySQL 5}. Vyd.~1. Překlad Jan Svoboda, Ondřej Baše, Jaroslav Černý. Brno: Computer Press, 2007, 805~s. ISBN 978-80-251-1502-2. 

\bibitem{HibernateSearchAction}
BERNARD, Emmanuel a John GRIFFIN. \textit{Hibernate search in action}. Greenwich, CT: Manning, c2009, xxiv, 463 p. ISBN 19-339-8864-9. 

\bibitem{LuceneAction}
MCCANDLESS, Michael, Erik HATCHER, Otis GOSPODNETIC a Otis GOSPODNETIC. \textit{Lucene in action}. 2nd ed. Greenwich: Manning, c2010, xxxviii, 488 p. ISBN 19-339-8817-7. 

\bibitem{LuceneWikiOnline}
Lucene FAQ. \textit{Lucene-java Wiki} [online]. 2004 [cit. 2014-05-04]. Dostupné z: \url{http://wiki.apache.org/lucene-java/LuceneFAQ}

\bibitem{ElasticsearchDefinitiveGuide}
\textit{Elasticsearch: The Definitive Guide} [online]. 2014 [cit. 2014-05-04]. Dostupné z: \url{http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/}

\bibitem{ElasticsearchWiki}
Elasticsearch. In: \textit{Wikipedia: the free encyclopedia} [online]. San Francisco (CA): Wikimedia Foundation, 2001- [cit. 2014-05-04]. Dostupné z: \url{http://en.wikipedia.org/wiki/Elasticsearch}

\bibitem{ProGit}
CHACON, Scott. \textit{Pro Git}. New York: Apress, c2009, xxi, 265~s. ISBN 978-1-4302-1833-3. 

\end{thebibliography}

\addtocontents{toc}{\setcounter{tocdepth}{2}}
\endgroup

%% Additional materials
\appendix

%% End of the whole document
\end{document}
